{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This notebook fine-tunes BERT on the IMDB positive/negative reviews dataset"
      ],
      "metadata": {
        "id": "Hb4t5j_qnqa_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This code will run faster on GPU, you could try Google Colab's free T4 GPU if needed\n",
        "### You can also freeze the backbone model to simply do transfer learning with the new head, for speed (Step 6)"
      ],
      "metadata": {
        "id": "gRqz3xN-AD_Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wqhsgbn3VKQY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ‚úÖ Step 0: Install required libraries (only run once)\n",
        "!pip install datasets -U transformers\n",
        "# You may need to add more packages here if module not found\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import the main tools for this project:  \n",
        "\n",
        "- `load_dataset` from **datasets** to load IMDB reviews.  \n",
        "- `BertTokenizer` and `BertForSequenceClassification` from **transformers** to preprocess text and load the model.  \n",
        "- `Trainer` and `TrainingArguments` to manage the training loop.  \n",
        "- **torch** for tensor operations.  \n",
        "- **numpy** for numerical utilities.  \n",
        "- **accuracy_score** from scikit-learn for evaluation.  \n"
      ],
      "metadata": {
        "id": "kvLX2JXgr9Ye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ‚úÖ Step 1: Imports\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "iWeERSfvVMg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the **IMDB dataset**, which is already split into train and test sets.  \n",
        "\n",
        "- Training: we take a random subset of 2,000 reviews.  \n",
        "- Testing: we take a random subset of 1,000 reviews.  \n",
        "\n",
        "Why subsets?  \n",
        "- The full IMDB dataset has 25,000 reviews per split.  \n",
        "- Subsets make training **much faster** while still showing the fine-tuning process.  \n",
        "\n",
        "We shuffle before selecting so that our subset is a fair mix of positive and negative reviews.  \n"
      ],
      "metadata": {
        "id": "2JpoFDSEr_R6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ‚úÖ Step 2: Load the IMDB dataset and take a small subset for quick training\n",
        "dataset = load_dataset(\"imdb\")\n",
        "small_train = dataset[\"train\"].shuffle(seed=42).select(range(2000))  # Random 2000 samples\n",
        "small_test = dataset[\"test\"].shuffle(seed=42).select(range(1000))  # Random 1000 samples\n",
        "# Note that the data comes already split to test train\n",
        "# The Train and Test were ordered, so we had to randomly select samples\n"
      ],
      "metadata": {
        "id": "NeAVCMX6VNVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the **pre-trained BERT tokenizer** (`bert-base-uncased`).  \n",
        "\n",
        "- Converts raw text into token IDs that BERT understands.  \n",
        "- Uses a subword vocabulary learned during BERT‚Äôs pre-training.  \n",
        "- Handles casing (lowercased text in this model).  \n",
        "\n",
        "The tokenizer is essential because BERT cannot process raw strings directly.  \n"
      ],
      "metadata": {
        "id": "spilhhJFsBJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ‚úÖ Step 3: Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# Loads the pre-trained BERT tokenizer; will convert raw text to token IDs that BERT understands\n"
      ],
      "metadata": {
        "id": "uRFCwbvQ-L8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a tokenisation function that:  \n",
        "- **Pads** sequences to the maximum length (so all inputs are the same size).  \n",
        "- **Truncates** longer reviews (BERT can only handle up to 512 tokens).  \n",
        "- Returns token IDs and attention masks.  \n",
        "\n",
        "We then apply this function to both the training and test sets using `map(batched=True)`, which processes multiple examples at once for speed.  \n"
      ],
      "metadata": {
        "id": "8Su2kX3OsCpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ‚úÖ Step 4: Tokenise text\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "# Defines a function to tokenize a batch of text: pads to max length, truncates if too long\n",
        "\n",
        "train_enc = small_train.map(tokenize, batched=True)\n",
        "test_enc = small_test.map(tokenize, batched=True)\n",
        "# Apply the tokenizer to the train/test datasets; batched=True for speed\n"
      ],
      "metadata": {
        "id": "MuyL28k8-PcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set the dataset format to `\"torch\"`, keeping only the columns that BERT needs:  \n",
        "- `input_ids` (the token IDs for each word/subword).  \n",
        "- `attention_mask` (marks which tokens are real vs. padding).  \n",
        "- `label` (the sentiment: 0 = negative, 1 = positive).  \n",
        "\n",
        "This ensures that the dataset can be fed directly into the Hugging Face **Trainer API**.  \n"
      ],
      "metadata": {
        "id": "PSPRKf-psEK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ‚úÖ Step 5: Set PyTorch format\n",
        "train_enc.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "test_enc.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "# Converts datasets to PyTorch tensors for use with Trainer\n"
      ],
      "metadata": {
        "id": "g0pF0JLR-Qt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6 is where we call in BERT as a pretrained model, with our additional 2-label classification head on top ‚úÖ‚úÖ‚úÖ"
      ],
      "metadata": {
        "id": "bu5oEIMY-mm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the **BERT model for sequence classification** (`bert-base-uncased`):  \n",
        "\n",
        "- It starts with the pretrained BERT backbone (which has learned language representations from large text corpora).  \n",
        "- We add a **classification head** (a small fully connected layer) for binary sentiment classification.  \n",
        "- `num_labels=2` because IMDB reviews are labelled as positive or negative.  \n",
        "\n",
        "This setup is now ready for training.  \n"
      ],
      "metadata": {
        "id": "1M132YDssF4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ‚úÖ Step 6: Load model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "# Loads pre-trained BERT **with a classification head**\n",
        "# num_labels=2 means binary sentiment classification\n"
      ],
      "metadata": {
        "id": "wEpOigad-RzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### Optional - if you want to freeze the backbone model and just train the appended head:\n",
        "# for param in model.bert.parameters():\n",
        "#     param.requires_grad = False\n",
        "### This will make the model train much faster\n"
      ],
      "metadata": {
        "id": "Uml-_tZVDu6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a function to calculate evaluation metrics during training.  \n",
        "\n",
        "- `eval_pred` contains model **logits** (raw outputs before softmax) and true labels.  \n",
        "- We take the **argmax** of the logits to get predicted class indices.  \n",
        "- Compute **accuracy** by comparing predictions to true labels.  \n",
        "\n",
        "This function is passed to the Hugging Face `Trainer` so it automatically calculates metrics at the end of each evaluation.  \n"
      ],
      "metadata": {
        "id": "tdPSVICssRKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ‚úÖ Step 7: Define metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
        "# Defines a metric function for Trainer to compute accuracy\n"
      ],
      "metadata": {
        "id": "CNwN0ZPW-SqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set up **TrainingArguments**, which control how the Trainer fine-tunes BERT:  \n",
        "\n",
        "- `output_dir`: where model checkpoints and logs are stored.  \n",
        "- `per_device_train_batch_size` / `per_device_eval_batch_size`: batch size per device (GPU/CPU).  \n",
        "- `num_train_epochs`: how many times the model will see the training dataset.  \n",
        "- `eval_strategy=\"epoch\"`: evaluate after each epoch.  \n",
        "- `save_strategy=\"no\"`: we skip saving intermediate checkpoints (saves time and space).  \n",
        "- `logging_steps`: print training metrics every 10 steps.  \n",
        "- `logging_dir`: directory for TensorBoard logs if needed.  \n",
        "- `report_to=\"none\"`: disables reporting to external tools like WandB.  \n",
        "\n",
        "These settings allow fast, clear, and reproducible fine-tuning on a small dataset.  \n"
      ],
      "metadata": {
        "id": "7-zV3D2BsSvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ‚úÖ Step 8: Define training configuration\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert-imdb\",            # Where to save model checkpoints and logs\n",
        "    per_device_train_batch_size=8,       # Number of samples per GPU/CPU batch during training\n",
        "    per_device_eval_batch_size=8,        # Number of samples per GPU/CPU batch during evaluation\n",
        "    num_train_epochs=2,                  # Total number of training epochs over the dataset\n",
        "    eval_strategy=\"epoch\",               # Evaluate model at the end of each epoch\n",
        "    save_strategy=\"no\",                  # Do not save intermediate model checkpoints\n",
        "    logging_steps=10,                    # Log training metrics every 10 steps\n",
        "    logging_dir=\"./logs\",                # Directory to store logs for TensorBoard if needed\n",
        "    load_best_model_at_end=False,        # Do not automatically reload the best model after training\n",
        "    report_to=\"none\",                    # Disable reporting to external tools like WandB\n",
        ")\n"
      ],
      "metadata": {
        "id": "H10mEeiu-Tak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If the following code runs very slowly, try a GPU. Google Colab has a free T4 GPU\n",
        "### Note that even with Colab's GPU, it may still take around 5 minutes to run\n",
        "### If you want it to run faster, go back and freeze the backbone model (Step 6)"
      ],
      "metadata": {
        "id": "qrER5FbrBMYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We initialise the **Hugging Face Trainer** with:  \n",
        "\n",
        "- `model`: the BERT model with classification head.  \n",
        "- `args`: the training arguments defined above.  \n",
        "- `train_dataset` / `eval_dataset`: tokenised train and test subsets.  \n",
        "- `compute_metrics`: the function to compute accuracy.  \n",
        "\n",
        "Then we call `trainer.train()`, which:  \n",
        "1. Loops over the dataset for the specified number of epochs.  \n",
        "2. Performs forward and backward passes.  \n",
        "3. Updates the model parameters (by default only the classification head if backbone is frozen).  \n",
        "\n",
        "After training, we move the model to **GPU (if available)** for faster inference.  \n"
      ],
      "metadata": {
        "id": "Jf-VBU6EsV-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ‚úÖ Step 9: Train\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_enc,\n",
        "    eval_dataset=test_enc,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "# Activate the Hugging Face Trainer with model, datasets, metrics, and args\n",
        "\n",
        "trainer.train()\n",
        "# Train the model (fine-tuning the head by default, backbone model can also be updated if not frozen)\n",
        "\n",
        "import torch\n",
        "\n",
        "# after trainer.train()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "# Moves the model to GPU if available for inference\n"
      ],
      "metadata": {
        "id": "a_U3gtLQ-UKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We evaluate the fine-tuned model on the test set:  \n",
        "\n",
        "- `trainer.evaluate()` computes the metrics (accuracy in this case) on the evaluation dataset.  \n",
        "- We print the test accuracy to get a quick sense of model performance.  \n",
        "\n",
        "This step ensures our model generalises well to unseen data.  \n"
      ],
      "metadata": {
        "id": "aFoJ7IKVsZld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ‚úÖ Step 10: Evaluate\n",
        "metrics = trainer.evaluate()\n",
        "print(\"‚úÖ Test Accuracy:\", metrics[\"eval_accuracy\"])\n",
        "# Compute evaluation metrics on the validation/test set\n"
      ],
      "metadata": {
        "id": "Z1G-vvRF-VTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a convenient function `predict_sentiment(text)` to classify new reviews:  \n",
        "\n",
        "1. **Tokenize the input** text (padding/truncating to 512 tokens).  \n",
        "2. Move the inputs to the **same device** as the model.  \n",
        "3. Set the model to **evaluation mode** (`model.eval()`) to disable dropout.  \n",
        "4. **Forward pass** through BERT, get logits, apply softmax for probabilities.  \n",
        "5. Select the **class with highest probability** and report its confidence.  \n",
        "6. Convert numeric prediction (0/1) to human-readable sentiment:  \n",
        "   - üëç Positive  \n",
        "   - üëé Negative  \n",
        "\n",
        "We then test the function on example sentences to see the predictions in action.  \n"
      ],
      "metadata": {
        "id": "lFJt3HE5sbEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ‚úÖ Step 11: Make a predictive function\n",
        "def predict_sentiment(text):\n",
        "    # Tokenise the input text into IDs BERT understands, add padding/truncation\n",
        "    # Convert to PyTorch tensors and move to the same device as the model (CPU or GPU)\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",   # return PyTorch tensors\n",
        "        truncation=True,       # cut off if longer than max_length\n",
        "        padding=True,          # pad shorter sequences\n",
        "        max_length=512         # maximum token length\n",
        "    ).to(device)               # move tensors to GPU if available\n",
        "\n",
        "    # Put model in evaluation mode (disables dropout, etc.)\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient calculation for faster inference and lower memory usage\n",
        "    with torch.no_grad():\n",
        "        # Forward pass through the model\n",
        "        outputs = model(**inputs)\n",
        "        # Get the raw logits (pre-softmax scores) from model output\n",
        "        logits = outputs.logits\n",
        "        # Convert logits to probabilities using softmax\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        # Pick the index with the highest probability as the predicted class\n",
        "        pred = torch.argmax(probs, dim=1).item()\n",
        "        # Get the confidence of the predicted class\n",
        "        confidence = probs[0, pred].item()\n",
        "\n",
        "    # Convert numeric prediction to human-readable sentiment\n",
        "    sentiment = \"üëç Positive\" if pred == 1 else \"üëé Negative\"\n",
        "    # Print the sentiment with its confidence percentage\n",
        "    print(f\"Sentiment: {sentiment} ({confidence:.2%} confidence)\")\n"
      ],
      "metadata": {
        "id": "sJUTkgad-WaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now take a look at our model in action:"
      ],
      "metadata": {
        "id": "C3Z78L6ZseyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Test predictions ‚úÖ‚úÖ‚úÖ\n",
        "predict_sentiment(\"I loved the movie.\")\n",
        "predict_sentiment(\"It was boring, slow, and way too long. I wouldn't recommend it.\")\n"
      ],
      "metadata": {
        "id": "c94duKFe-XCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we use longer movie reviews as test data, we'll see that our model outperforms other off-the-shelf binary sentiment classification models  \n",
        "\n",
        "In fact, let's compare our results with the original BERT model. BERT is great at understanding, but hasn't been trained for binary classification at all -->"
      ],
      "metadata": {
        "id": "0Kpak8udDVvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BERT with fresh classifier head\n",
        "from transformers import BertForSequenceClassification\n",
        "untrained_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "untrained_model.to(device)\n",
        "\n",
        "# Quick predict function\n",
        "def predict_raw(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(device)\n",
        "    untrained_model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = untrained_model(**inputs).logits\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        pred = logits.argmax(dim=1).item()\n",
        "        confidence = probs[0, pred].item()\n",
        "    sentiment = \"üëç Positive\" if pred == 1 else \"üëé Negative\"\n",
        "    print(f\"Sentiment: {sentiment} ({confidence:.2%} confidence)\")\n",
        "\n",
        "# Test\n",
        "predict_raw(\"I loved the movie.\")\n",
        "predict_raw(\"It was boring, slow, and way too long. I wouldn't recommend it.\")\n"
      ],
      "metadata": {
        "id": "SOpECjWGE0Tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how the default BERT model is not good at this task, because it has not been trained on it yet."
      ],
      "metadata": {
        "id": "GrtL0zSzFFfV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nv6OJ4AOFKoP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}